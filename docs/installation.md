# Get started

This sections covers how to get started with `ConfigVLM`.

`ConfigVLM` allows you to easily combine and use predefined vision and language models and use them for tasks such as [VQA](vqa.md) or [Image captioning](image_captioning.md).
For this, models can be trained in an end-to-end fashion or pre-trained checkpoints can be used.
Since the amount of well-annotated data for complex tasks often fluctuates, the framework also offers the possibility to pre-train the vision models in a supervised fashion.
