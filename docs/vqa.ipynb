{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Visual Question Answering\n",
    "The Framework is set up in way, that it is easy to combine a vision model from the [timm](https://github.com/huggingface/pytorch-image-models/tree/main/timm/models) library with a language model from [huggingface](https://huggingface.co/). For both models, either pre-trained weights can be used or the models can be trained as a composite in an end-to-end fashion.\n",
    "For this example usage we will be using the [`RSVQAxBEN DataModule`](extra/rsvqaxben.ipynb) from [1] inside a [`pytorch lightning`](https://pytorch-lightning.readthedocs.io/en/stable/) trainer. The network will be integrated into a [`LightningModule`](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html) to release us from writing training loop etc.\n",
    "[1] [RSVQA Meets Bigearthnet: A New, Large-Scale, Visual Question Answering Dataset for Remote Sensing](https://ieeexplore.ieee.org/document/9553307)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we start by importing the basics we need from `torch` and `pytorch_lightning` that are needed to set up the `LightningModule`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lhackel/Documents/development/ConfigVLM/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# remove-output\n",
    "# remove-input\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from configvlm import ConfigVLM"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch Lightning Module\n",
    "The `Module` we use to encapsulate the model divides the usual loop into functions that are called internally by `pytorch_lightning`. The necessary functions are just `training_step` and `configure_optimizer`, but to have a fully functional script, we add the validation and test steps as well as evaluation of the validation and test results. All `_step` functions are working on a single batch while `_epoch_end` functions are called after all batches are used and are passed a list of all return values of their respective `_step` functions.\n",
    "For VQA we have to add one additional function, as the network works with 3 values (vision + language input, output) instead of the usual 2 (input, output). Therefore we add a function (here called `_disassemble_batch`), which disassembles the batch into input and output where the _input contains both modalities_."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class LitVQAEncoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ConfigVLM.VLMConfiguration,\n",
    "        lr: float = 1e-3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.config = config\n",
    "        self.model = ConfigVLM.ConfigVLM(config)\n",
    "\n",
    "    def _disassemble_batch(self, batch):\n",
    "        images, questions, labels = batch\n",
    "        # For some reason questions come in here transposed as a list of Tensors\n",
    "        # where the first elements of the question are in the first tensor (first\n",
    "        # element of the list), all the second elements are in the second tensor\n",
    "        # which is the second element of the list and so on.\n",
    "        # So we first make it a list of lists and then a big tensor and then\n",
    "        # transpose this tensor.\n",
    "        # Now each tensor contains one question\n",
    "        questions = torch.tensor(\n",
    "            [x.tolist() for x in questions], device=self.device\n",
    "        ).T.int()\n",
    "        return (images, questions), labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = self._disassemble_batch(batch)\n",
    "        x_hat = self.model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(x_hat, y)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "        return optimizer\n",
    "\n",
    "    # ============== NON-MANDATORY-FUNCTION ===============\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = self._disassemble_batch(batch)\n",
    "        x_hat = self.model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(x_hat, y)\n",
    "        return {\"loss\": loss, \"outputs\": x_hat, \"labels\": y}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"val/loss\", avg_loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = self._disassemble_batch(batch)\n",
    "        x_hat = self.model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(x_hat, y)\n",
    "        return {\"loss\": loss, \"outputs\": x_hat, \"labels\": y}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"test/loss\", avg_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuring\n",
    "Now that we have our model, we will use the `pytorch_lightning.Trainer` to run our loops. Results are logged to `tensorboard`.\n",
    "\n",
    "We start by importing some callbacks used during training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from configvlm.ConfigVLM import VLMConfiguration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "as well as defining our hyperparameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "vision_model_name = \"resnet18\"\n",
    "text_model_name = \"prajjwal1/bert-tiny\"\n",
    "seed = 42\n",
    "number_of_channels = 12\n",
    "image_size = 120\n",
    "epochs = 4\n",
    "lr = 5e-4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we create the configuration for usage in model creation later and the logger."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# remove-output\n",
    "# seed for pytorch, numpy, python.random, Dataloader workers, spawned subprocesses\n",
    "pl.seed_everything(seed, workers=True)\n",
    "\n",
    "model_config = VLMConfiguration(\n",
    "    timm_model_name=vision_model_name,\n",
    "    hf_model_name=text_model_name,  # different to pre-training\n",
    "    classes=1000,  # different to pre-training\n",
    "    image_size=image_size,\n",
    "    channels=number_of_channels,\n",
    "    network_type=ConfigVLM.VLMType.VQA_CLASSIFICATION  # different to pre-training\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"./tb_logs\",\n",
    "    name=\"VQA Test Model\",\n",
    "    version=\"testversion\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We log the hyperparameters and create a [Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# remove-output\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"auto\",\n",
    "    logger=logger,\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "logger.log_hyperparams({\n",
    "    \"Model Name\": \"VQA Test Model\",\n",
    "    \"Seed\": seed,\n",
    "    \"Epochs\": epochs,\n",
    "    \"Channels\": number_of_channels,\n",
    "    \"Image Size\": image_size,\n",
    "    \"GPU\": torch.cuda.get_device_name() if torch.cuda.is_available() else \"-\",\n",
    "    \"Learning Rate\": lr,\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Model + Dataset\n",
    "Finally, we create the model defined above and our datamodule. We will be using a datamodule from this framework described in the Extra section."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# remove-input\n",
    "# remove-output\n",
    "import pathlib\n",
    "my_data_path = str(pathlib.Path(\"\").resolve().parent.joinpath(\"configvlm\").joinpath(\"extra\").joinpath(\"mock_data\").resolve(strict=True))\n",
    "# set precision on Ampere cards to bfloat16\n",
    "torch.set_float32_matmul_precision('medium')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lhackel/Documents/development/ConfigVLM/configvlm/ConfigVLM.py:131: UserWarning: Keyword 'img_size' unknown. Trying to ignore and restart creation.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/lhackel/.cache/configvlm/pretrained_models/huggingface_models/prajjwal1/bert-tiny were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/lhackel/Documents/development/ConfigVLM/configvlm/ConfigVLM.py:115: UserWarning: Tokenizer was initialized pretrained\n",
      "  warnings.warn(\"Tokenizer was initialized pretrained\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader using 4 workers\n",
      "\n",
      "\u001B[96mHINT: pin_memory set to None \u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# hide-output\n",
    "from configvlm.extra.RSVQAxBEN_DataModule_LMDB_Encoder import RSVQAxBENDataModule\n",
    "from configvlm.ConfigVLM import get_hf_model\n",
    "model = LitVQAEncoder(config=model_config, lr=lr)\n",
    "dm = RSVQAxBENDataModule(\n",
    "    data_dir=my_data_path,\n",
    "    img_size=(number_of_channels, image_size, image_size),\n",
    "    num_workers_dataloader=4,\n",
    "    tokenizer = get_hf_model(model_name=text_model_name)[0]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running\n",
    "Now we just have to call the `fit()` and optionally the `test()` functions.\n",
    "\n",
    ":::{note}\n",
    "These calls generate quite a bit of output depending on the number of batches and epochs. The output is removed for readability.\n",
    ":::"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11:07:59) Datamodule setup called\n",
      "Loading split RSVQAxBEN data for train...\n",
      "              25 QA-pairs indexed\n",
      "              25 QA-pairs in reduced data set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Answers: 100%|██████████| 25/25 [00:00<00:00, 346064.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[93mWARNING: There are fewer possible answers then requested (1000 requested, but 4 found).\u001B[0m\n",
      "The 1000 most frequent answers cover about 100.00 % of the total answers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to NP arrays: 100%|██████████| 25/25 [00:00<00:00, 845625.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split RSVQAxBEN data for val...\n",
      "              25 QA-pairs indexed\n",
      "              25 QA-pairs in reduced data set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to NP arrays: 100%|██████████| 25/25 [00:00<00:00, 474468.78it/s]\n",
      "/home/lhackel/Documents/development/ConfigVLM/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory ./tb_logs/VQA Test Model/testversion/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup took 0.01 seconds\n",
      "  Total training samples:       25  Total validation samples:       25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | ConfigVLM | 16.6 M\n",
      "------------------------------------\n",
      "16.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "16.6 M    Total params\n",
      "66.281    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 2/4 [00:00<00:00,  3.15it/s, loss=0.691, v_num=sion]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  75%|███████▌  | 3/4 [00:00<00:00,  3.35it/s, loss=0.691, v_num=sion]\n",
      "Epoch 0: 100%|██████████| 4/4 [00:00<00:00,  4.41it/s, loss=0.691, v_num=sion]\n",
      "Epoch 1:  50%|█████     | 2/4 [00:00<00:00,  5.09it/s, loss=0.683, v_num=sion]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  75%|███████▌  | 3/4 [00:00<00:00,  4.69it/s, loss=0.683, v_num=sion]\n",
      "Epoch 1: 100%|██████████| 4/4 [00:00<00:00,  6.15it/s, loss=0.683, v_num=sion]\n",
      "Epoch 2:  50%|█████     | 2/4 [00:00<00:00,  6.18it/s, loss=0.666, v_num=sion]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:  75%|███████▌  | 3/4 [00:00<00:00,  5.03it/s, loss=0.666, v_num=sion]\n",
      "Epoch 2: 100%|██████████| 4/4 [00:00<00:00,  6.56it/s, loss=0.666, v_num=sion]\n",
      "Epoch 3:  50%|█████     | 2/4 [00:00<00:00,  5.65it/s, loss=0.639, v_num=sion]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  75%|███████▌  | 3/4 [00:00<00:00,  4.96it/s, loss=0.639, v_num=sion]\n",
      "Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  6.46it/s, loss=0.639, v_num=sion]\n",
      "Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  6.42it/s, loss=0.639, v_num=sion]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 4/4 [00:00<00:00,  4.25it/s, loss=0.639, v_num=sion]\n"
     ]
    }
   ],
   "source": [
    "# hide-output\n",
    "trainer.fit(model, datamodule=dm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11:08:08) Datamodule setup called\n",
      "Loading split RSVQAxBEN data for test...\n",
      "              25 QA-pairs indexed\n",
      "              25 QA-pairs in reduced data set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to NP arrays: 100%|██████████| 25/25 [00:00<00:00, 421114.86it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup took 0.00 seconds\n",
      "  Total test samples:       25\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 100.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m        test/loss        \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.5322797298431396    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5322797298431396     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'test/loss': 0.5322797298431396}]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide-output\n",
    "trainer.test(model, datamodule=dm)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
